# 多模态模型预训练中细颗粒度对齐问题
细颗粒度的对齐问题是多模态模型预训练的核心问题，主要是需要在不同模态的表征空间里进行更细致的对齐。我的理解是要从不同模态的知识表示本质来探索，目前多是用几个固定的预训练方法，比如image-text对比loss，还有自监督的generation-mask任务。有可能最终还是要走到对基础网络架构的创新，出现一个崭新的类似transformers的模型或者是对transformers模型的基本理论突破。
主要就是针对某个模型提出一个达到更好对齐效果的方法，或者是针对某个模型的结构进行改进，增强对齐效果，然后在下游任务上做验证。

我觉得这一块是是需要结合很多别的部分的知识的，对比学习，表征学习，自监督，还有一些知识图谱。只有内容看的足够广，才能有所借鉴与创新。


### [知乎上该方向的基本介绍](https://zhuanlan.zhihu.com/p/476655482/)
